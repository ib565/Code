{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"100206\"\n",
    "subject_path = f\"diffusion_data/{subject_id}/T1w/Diffusion\"\n",
    "dwi_img = nib.load(f'{subject_path}/data.nii.gz')\n",
    "mask_img = nib.load(f'{subject_path}/nodif_brain_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwi_data = dwi_img.get_fdata()\n",
    "original_mask = mask_img.get_fdata() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store original indices before any voxel filtering \n",
    "original_idx = np.where(original_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map (x,y,z) to original linear index\n",
    "coord_to_gtt = {}\n",
    "for i in range(len(original_idx[0])):\n",
    "    x, y, z = original_idx[0][i], original_idx[1][i], original_idx[2][i]\n",
    "    coord_to_gtt[(x,y,z)] = i  # i is the index in ground_truth_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWI data shape: (145, 174, 145, 288)\n"
     ]
    }
   ],
   "source": [
    "print(f\"DWI data shape: {dwi_data.shape}\")  # (X, Y, Z, num_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvals, bvecs = read_bvals_bvecs(f'{subject_path}/bvals', \n",
    "                               f'{subject_path}/bvecs')\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B0 data shape: (145, 174, 145, 18)\n",
      "Number of b=1000 volumes: 90\n"
     ]
    }
   ],
   "source": [
    "# get b0 data and b0_avg\n",
    "b0_mask = gtab.b0s_mask\n",
    "b0_data = dwi_data[..., b0_mask]\n",
    "print(f\"B0 data shape: {b0_data.shape}\")\n",
    "b0_avg = np.mean(b0_data, axis=-1)\n",
    "\n",
    "# b1000 images mask\n",
    "b1000_mask = (bvals >= 990) & (bvals <= 1010)\n",
    "print(f\"Number of b=1000 volumes: {np.sum(b1000_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWI volumes shape: (145, 174, 145, 90)\n"
     ]
    }
   ],
   "source": [
    "# Get all b1000 scans\n",
    "dwi_vols = dwi_data[..., b1000_mask]\n",
    "print(f\"DWI volumes shape: {dwi_vols.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask out voxels with very low b0 signal\n",
    "b0_threshold = 250\n",
    "valid_b0_mask = b0_avg > b0_threshold\n",
    "mask = original_mask & valid_b0_mask  # Combine with brain mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of valid voxels in mask: 926671\n"
     ]
    }
   ],
   "source": [
    "# Find valid voxels using the brain mask\n",
    "valid_idx = np.where(mask) # shape: 3 x num_valid_voxels\n",
    "print(f\"\\nNumber of valid voxels in mask: {len(valid_idx[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random voxels for training\n",
    "n_samples = 150000\n",
    "sample_idx = np.random.choice(len(valid_idx[0]), \n",
    "                            min(n_samples, len(valid_idx[0])), \n",
    "                            replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (signal intensities) from sampled voxels\n",
    "features = []\n",
    "gtt_indices = [] # store ground truth tensor indices. gtt_indices[i] gives index of ith feature in original number of voxels (93XXXX)\n",
    "\n",
    "for idx in sample_idx:\n",
    "    x, y, z = valid_idx[0][idx], valid_idx[1][idx], valid_idx[2][idx]\n",
    "    signal = dwi_vols[x, y, z, :]\n",
    "    b0_ref = b0_avg[x, y, z]\n",
    "    normalized_signal = signal / (b0_ref)\n",
    "    features.append(normalized_signal)\n",
    "    gtt_indices.append(coord_to_gtt[(x, y, z)])\n",
    "\n",
    "# gtt_indices shape = 100000 x 1\n",
    "# features shape = 100000 x 90 x 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "gtt_indices = np.array(gtt_indices)\n",
    "gradient_directions = bvecs[b1000_mask]  # Only keep directions for DWI volumes, shape = 90 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading ground truth data...\")\n",
    "gt_data = np.load('ground_truth_v2.npz')\n",
    "ground_truth_tensors = gt_data['tensors'] # shape 936256(all voxels) x 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936256, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_directions = 21\n",
    "\n",
    "def create_graph_data(batch_indices, direction_indices, batch_size=32, threshold_angle=60):\n",
    "    \"\"\"\n",
    "    Optimized version of create_graph_data that processes the batch in parallel\n",
    "    using vectorized operations.\n",
    "    \"\"\"\n",
    "    # Pre-compute directions once for the whole batch\n",
    "    current_directions = gradient_directions[direction_indices]  # shape = 21 x 3\n",
    "    directions_norm = current_directions / np.linalg.norm(current_directions, axis=1, keepdims=True)\n",
    "    \n",
    "    # Pre-compute angle matrix once\n",
    "    cos_sim = np.dot(directions_norm, directions_norm.T)\n",
    "    angles = np.arccos(np.clip(cos_sim, -1.0, 1.0)) * 180/np.pi\n",
    "    src, dst = np.where(angles < threshold_angle)\n",
    "    mask = src != dst\n",
    "    edge_template = np.column_stack([src[mask], dst[mask]])\n",
    "    edge_weights_template = cos_sim[src[mask], dst[mask]]\n",
    "    \n",
    "    # Process all signals at once\n",
    "    all_signals = features[batch_indices][:, direction_indices]  # shape = batch_size x 21 x 1\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    batch_nodes = np.zeros((len(batch_indices), n_directions, 4))\n",
    "    batch_edges = [edge_template for _ in range(len(batch_indices))]\n",
    "    batch_edge_weights = [edge_weights_template for _ in range(len(batch_indices))]\n",
    "    \n",
    "    # Vectorized node creation\n",
    "    batch_nodes[..., 0] = all_signals.reshape(len(batch_indices), -1)\n",
    "    batch_nodes[..., 1:4] = current_directions[None, :, :]  # Broadcasting\n",
    "    \n",
    "    # Get tensors in one operation\n",
    "    batch_tensors = ground_truth_tensors[gtt_indices[batch_indices]]\n",
    "    \n",
    "    return batch_nodes, batch_edges, batch_edge_weights, batch_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_diverse_directions(all_directions, n_select=21):\n",
    "    selected = [0]  # Start with first direction\n",
    "    while len(selected) < n_select:\n",
    "        # Calculate angles with all selected directions\n",
    "        angles = []\n",
    "        for i in range(len(all_directions)):\n",
    "            if i in selected:\n",
    "                continue\n",
    "            min_angle = float('inf')\n",
    "            for s in selected:\n",
    "                # Cosine similarity\n",
    "                angle = np.arccos(np.clip(\n",
    "                    np.dot(all_directions[i], all_directions[s]), -1.0, 1.0))\n",
    "                min_angle = min(min_angle, angle)\n",
    "            angles.append((i, min_angle))\n",
    "        # Select direction with largest minimum angle\n",
    "        next_idx = max(angles, key=lambda x: x[1])[0]\n",
    "        selected.append(next_idx)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_direction_sets(all_directions, n_sets=3, n_directions=21):\n",
    "    base_sets = []\n",
    "    n_total = len(all_directions)\n",
    "    \n",
    "    for i in range(n_sets):\n",
    "        # Create a shuffled index array for this set\n",
    "        shuffled_indices = np.random.permutation(n_total)\n",
    "        # Use select_diverse_directions on the shuffled indices\n",
    "        selected = select_diverse_directions(all_directions[shuffled_indices], n_directions)\n",
    "        # Map back to original indices\n",
    "        original_indices = shuffled_indices[selected]\n",
    "        base_sets.append(original_indices)\n",
    "    \n",
    "    return base_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, Batch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features = 4, hidden_dim = 128):\n",
    "        super(DiffusionGNN, self).__init__()\n",
    "    \n",
    "        # Layers\n",
    "        self.conv1 = DiffusionConv(node_features, hidden_dim)\n",
    "        self.conv2 = DiffusionConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # MLP for tensor prediction\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 6),\n",
    "        )\n",
    "\n",
    "        self.register_buffer('loss_weights',\n",
    "            torch.tensor([10, 500, 10, 500, 500, 10]))\n",
    "\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \"\"\"\n",
    "        x: node_features\n",
    "        edge_index = graph conn info [2, num_edges]\n",
    "        batch: batch assignment for nodes\n",
    "        \"\"\"\n",
    "        # print(\"pre conv\", x.shape)\n",
    "        # x shape = 672 (batch_size*num_directions) x 4\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # x shape = 672 x hidden_dim(64)\n",
    "        # print(\"after conv\", x.shape)\n",
    "\n",
    "        # combine node features for graph\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_dim]. 1 representation for each voxel/graph\n",
    "        # print(\"after pool\", x.shape)\n",
    "\n",
    "        # raw predictions\n",
    "        out = self.mlp(x) # shape = 32 x 6\n",
    "\n",
    "        diag_idx = [0, 2, 5]\n",
    "        offdiag_idx = [1, 3, 4]\n",
    "\n",
    "        # Diagonal elements: [0,1] using sigmoid\n",
    "        diag = torch.sigmoid(out[:, diag_idx])\n",
    "        \n",
    "        # Off-diagonal elements: [-1,1] using tanh\n",
    "        offdiag = torch.tanh(out[:, offdiag_idx])\n",
    "\n",
    "        out_reordered = torch.zeros_like(out)\n",
    "        out_reordered[:, diag_idx] = diag\n",
    "        out_reordered[:, offdiag_idx] = offdiag\n",
    "\n",
    "        return out_reordered # shape = 32 x 6\n",
    "    \n",
    "    def weighted_mse_loss(self, pred, target):\n",
    "        squared_diff = (pred - target) ** 2  # [batch_size, 6]\n",
    "        weighted_diff = squared_diff * self.loss_weights\n",
    "        return torch.mean(weighted_diff)\n",
    "\n",
    "\n",
    "class DiffusionConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DiffusionConv, self).__init__(aggr=\"mean\")\n",
    "        \n",
    "        # MLP to process messages\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * in_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        return self.propagate(edge_index, x=x, edge_weight = edge_weight)\n",
    "    \n",
    "    def message(self, x_i, x_j, edge_weight):\n",
    "        \"\"\"\n",
    "        x_i: features of target nodes\n",
    "        x_j: features of source nodes\n",
    "        Returns: messages to be aggregated\n",
    "        \"\"\"\n",
    "        tmp = torch.cat([x_i, x_j], dim = 1)\n",
    "        return self.mlp(tmp) * edge_weight.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_geometric(nodes, edges, edge_weights, tensors):\n",
    "    data_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        # Ensure tensor is properly shaped [6] not flattened\n",
    "        tensor = tensors[i].reshape(-1, 6) if len(tensors[i].shape) == 1 else tensors[i]\n",
    "        \n",
    "        data = Data(\n",
    "            x=torch.FloatTensor(nodes[i]),          # [n_nodes]\n",
    "            edge_index=torch.LongTensor(edges[i].T), # [2, n_edges]\n",
    "            edge_attr=torch.FloatTensor(edge_weights[i]),\n",
    "            y=torch.FloatTensor(tensor)             # [1, 6]\n",
    "        )\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 120000 voxels, testing with 30000 voxels\n"
     ]
    }
   ],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(len(features)), test_size=0.2, random_state=42)\n",
    "print(f\"Training with {len(train_idx)} voxels, testing with {len(test_idx)} voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pred, gt, epoch, print_results=True):\n",
    "    \"\"\"\n",
    "    Compute and optionally print evaluation metrics\n",
    "    Args:\n",
    "        pred: predictions array (n_samples x 6)\n",
    "        gt: ground truth array (n_samples x 6)\n",
    "        epoch: current epoch number\n",
    "        print_results: whether to print metrics\n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Component-wise statistics and relative errors\n",
    "    for i in range(6):\n",
    "        # Relative error\n",
    "        rel_error = np.mean(np.abs(pred[:,i] - gt[:,i]) / (np.abs(gt[:,i]) + 1e-10))\n",
    "        metrics[f'rel_error_{i}'] = rel_error\n",
    "        \n",
    "        # Statistics for monitoring distribution\n",
    "        metrics[f'pred_mean_{i}'] = np.mean(pred[:,i])\n",
    "        metrics[f'pred_std_{i}'] = np.std(pred[:,i])\n",
    "        metrics[f'gt_mean_{i}'] = np.mean(gt[:,i])\n",
    "        metrics[f'gt_std_{i}'] = np.std(gt[:,i])\n",
    "    \n",
    "    # Mean Diffusivity error\n",
    "    pred_md = np.mean([pred[:,0], pred[:,2], pred[:,5]], axis=0)  # Dxx, Dyy, Dzz\n",
    "    gt_md = np.mean([gt[:,0], gt[:,2], gt[:,5]], axis=0)\n",
    "    metrics['md_rel_error'] = np.mean(np.abs(pred_md - gt_md) / (np.abs(gt_md) + 1e-10))\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"\\nEpoch {epoch+1} Statistics:\")\n",
    "        print(\"\\nRelative Errors per component:\")\n",
    "        for i in range(6):\n",
    "            print(f\"Component {i}: {metrics[f'rel_error_{i}']:.2e}\")\n",
    "        print(f\"Mean Diffusivity Error: {metrics['md_rel_error']:.2e}\")\n",
    "        \n",
    "        print(\"\\nPrediction Statistics:\")\n",
    "        for i in range(6):\n",
    "            print(f\"Component {i}: mean={metrics[f'pred_mean_{i}']:.2e}, std={metrics[f'pred_std_{i}']:.2e}\")\n",
    "        \n",
    "        print(\"\\nGround Truth Statistics:\")\n",
    "        for i in range(6):\n",
    "            print(f\"Component {i}: mean={metrics[f'gt_mean_{i}']:.2e}, std={metrics[f'gt_std_{i}']:.2e}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sets = 3\n",
    "direction_sets = initialize_direction_sets(gradient_directions, n_sets)\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cache creation before training\n",
    "# print(\"Pre-computing graph data for all direction sets...\")\n",
    "# cached_samples = {}  # Dictionary: direction_set_idx -> list of preprocessed samples\n",
    "\n",
    "# # Pre-compute individual samples for each direction set\n",
    "# for set_idx, direction_set in enumerate(direction_sets):\n",
    "#     print(f\"Processing direction set {set_idx + 1}/{len(direction_sets)}\")\n",
    "#     cached_samples[set_idx] = []\n",
    "    \n",
    "#     # Process each training sample individually\n",
    "#     for idx in train_idx:  # train_idx shape: (80,000,)\n",
    "#         # Create graph data for single sample\n",
    "#         nodes, edges, edge_weights, tensors = create_graph_data(\n",
    "#             [idx],  # Single sample\n",
    "#             direction_set  # Shape: (21, 3)\n",
    "#         )\n",
    "#         # Convert to PyG data object\n",
    "#         data = convert_to_torch_geometric(nodes, edges, edge_weights, tensors)[0]  # Take first (only) element\n",
    "#         # Move to GPU and store\n",
    "#         cached_samples[set_idx].append(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 1/60, Loss: 22311.248545\n",
      "\n",
      "Epoch 1 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 6.21e+03\n",
      "Component 1: 1.85e+04\n",
      "Component 2: 7.11e+03\n",
      "Component 3: 2.25e+04\n",
      "Component 4: 3.55e+04\n",
      "Component 5: 7.09e+03\n",
      "Mean Diffusivity Error: 6.79e+03\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.25e-02, std=5.93e-02\n",
      "Component 1: mean=1.51e-04, std=3.93e-03\n",
      "Component 2: mean=1.45e-02, std=6.21e-02\n",
      "Component 3: mean=-1.06e-04, std=3.59e-03\n",
      "Component 4: mean=1.20e-04, std=3.97e-03\n",
      "Component 5: mean=1.43e-02, std=5.76e-02\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 2/60, Loss: 220.483912\n",
      "\n",
      "Epoch 2 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 1.97e+03\n",
      "Component 1: 9.39e+03\n",
      "Component 2: 2.26e+03\n",
      "Component 3: 9.89e+03\n",
      "Component 4: 1.07e+04\n",
      "Component 5: 2.29e+03\n",
      "Mean Diffusivity Error: 2.17e+03\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=2.66e-03, std=7.75e-04\n",
      "Component 1: mean=-8.08e-07, std=7.93e-04\n",
      "Component 2: mean=3.04e-03, std=8.87e-04\n",
      "Component 3: mean=2.65e-06, std=9.35e-04\n",
      "Component 4: mean=-2.27e-05, std=9.18e-04\n",
      "Component 5: mean=3.03e-03, std=8.60e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 3/60, Loss: 97.762649\n",
      "\n",
      "Epoch 3 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 1.35e+03\n",
      "Component 1: 5.55e+03\n",
      "Component 2: 1.51e+03\n",
      "Component 3: 7.25e+03\n",
      "Component 4: 6.41e+03\n",
      "Component 5: 1.51e+03\n",
      "Mean Diffusivity Error: 1.46e+03\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.94e-03, std=5.60e-04\n",
      "Component 1: mean=-8.04e-07, std=5.15e-04\n",
      "Component 2: mean=2.16e-03, std=6.21e-04\n",
      "Component 3: mean=3.01e-06, std=6.21e-04\n",
      "Component 4: mean=-2.27e-05, std=6.31e-04\n",
      "Component 5: mean=2.12e-03, std=5.91e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 4/60, Loss: 41.164862\n",
      "\n",
      "Epoch 4 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 9.06e+02\n",
      "Component 1: 4.00e+03\n",
      "Component 2: 9.89e+02\n",
      "Component 3: 5.14e+03\n",
      "Component 4: 4.69e+03\n",
      "Component 5: 9.84e+02\n",
      "Mean Diffusivity Error: 9.58e+02\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.51e-03, std=3.93e-04\n",
      "Component 1: mean=-1.21e-06, std=3.34e-04\n",
      "Component 2: mean=1.64e-03, std=4.21e-04\n",
      "Component 3: mean=2.90e-06, std=4.15e-04\n",
      "Component 4: mean=-2.27e-05, std=4.06e-04\n",
      "Component 5: mean=1.60e-03, std=3.97e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 5/60, Loss: 18.948934\n",
      "\n",
      "Epoch 5 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 6.30e+02\n",
      "Component 1: 3.28e+03\n",
      "Component 2: 6.76e+02\n",
      "Component 3: 3.72e+03\n",
      "Component 4: 3.94e+03\n",
      "Component 5: 6.75e+02\n",
      "Mean Diffusivity Error: 6.60e+02\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.28e-03, std=4.23e-04\n",
      "Component 1: mean=-9.71e-07, std=2.46e-04\n",
      "Component 2: mean=1.36e-03, std=4.41e-04\n",
      "Component 3: mean=3.19e-06, std=2.85e-04\n",
      "Component 4: mean=-2.28e-05, std=2.77e-04\n",
      "Component 5: mean=1.32e-03, std=4.15e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 6/60, Loss: 9.630379\n",
      "\n",
      "Epoch 6 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 5.70e+02\n",
      "Component 1: 2.48e+03\n",
      "Component 2: 6.10e+02\n",
      "Component 3: 2.31e+03\n",
      "Component 4: 2.21e+03\n",
      "Component 5: 6.12e+02\n",
      "Mean Diffusivity Error: 5.96e+02\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.14e-03, std=4.73e-04\n",
      "Component 1: mean=-1.14e-06, std=1.86e-04\n",
      "Component 2: mean=1.19e-03, std=4.81e-04\n",
      "Component 3: mean=3.27e-06, std=2.11e-04\n",
      "Component 4: mean=-2.28e-05, std=2.07e-04\n",
      "Component 5: mean=1.16e-03, std=4.53e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 7/60, Loss: 6.110667\n",
      "\n",
      "Epoch 7 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 5.60e+02\n",
      "Component 1: 1.69e+03\n",
      "Component 2: 6.01e+02\n",
      "Component 3: 1.86e+03\n",
      "Component 4: 1.90e+03\n",
      "Component 5: 6.04e+02\n",
      "Mean Diffusivity Error: 5.87e+02\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.05e-03, std=4.88e-04\n",
      "Component 1: mean=-1.17e-06, std=1.57e-04\n",
      "Component 2: mean=1.09e-03, std=4.88e-04\n",
      "Component 3: mean=3.35e-06, std=1.74e-04\n",
      "Component 4: mean=-2.29e-05, std=1.74e-04\n",
      "Component 5: mean=1.07e-03, std=4.64e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Epoch 8/60, Loss: 4.236937\n",
      "\n",
      "Epoch 8 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 6.49e+02\n",
      "Component 1: 1.52e+03\n",
      "Component 2: 6.94e+02\n",
      "Component 3: 1.62e+03\n",
      "Component 4: 1.41e+03\n",
      "Component 5: 6.86e+02\n",
      "Mean Diffusivity Error: 6.75e+02\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=1.02e-03, std=4.80e-04\n",
      "Component 1: mean=-1.17e-06, std=1.43e-04\n",
      "Component 2: mean=1.06e-03, std=4.79e-04\n",
      "Component 3: mean=3.36e-06, std=1.52e-04\n",
      "Component 4: mean=-2.29e-05, std=1.54e-04\n",
      "Component 5: mean=1.05e-03, std=4.65e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.90e-04, std=5.52e-04\n",
      "Component 1: mean=-1.21e-06, std=1.15e-04\n",
      "Component 2: mean=1.03e-03, std=5.58e-04\n",
      "Component 3: mean=3.41e-06, std=1.16e-04\n",
      "Component 4: mean=-2.29e-05, std=1.20e-04\n",
      "Component 5: mean=1.00e-03, std=5.65e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create graph data for batch\u001b[39;00m\n\u001b[0;32m     28\u001b[0m nodes, edges, edge_weights, tensors \u001b[38;5;241m=\u001b[39m create_graph_data(batch_indices, direction_indices)\n\u001b[1;32m---> 29\u001b[0m data_list \u001b[38;5;241m=\u001b[39m convert_to_torch_geometric(nodes, edges, edge_weights, tensors)\n\u001b[0;32m     30\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(data_list)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 11\u001b[0m, in \u001b[0;36mconvert_to_torch_geometric\u001b[1;34m(nodes, edges, edge_weights, tensors)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(nodes)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Ensure tensor is properly shaped [6] not flattened\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensors[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors[i]\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m tensors[i]\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m Data(\n\u001b[0;32m      8\u001b[0m         x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mFloatTensor(nodes[i]),          \u001b[38;5;66;03m# [n_nodes]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         edge_index\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mLongTensor(edges[i]\u001b[38;5;241m.\u001b[39mT), \u001b[38;5;66;03m# [2, n_edges]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         edge_attr\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mFloatTensor(edge_weights[i]),\n\u001b[1;32m---> 11\u001b[0m         y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mFloatTensor(tensor)             \u001b[38;5;66;03m# [1, 6]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_list\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = DiffusionGNN(hidden_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "epochs = 60\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Select direction set for this epoch\n",
    "    direction_indices = direction_sets[epoch % len(direction_sets)]\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Store epoch predictions and ground truth\n",
    "    epoch_pred_values = []\n",
    "    epoch_gt_values = []\n",
    "    \n",
    "    # Shuffle training indices\n",
    "    np.random.shuffle(train_idx)\n",
    "    \n",
    "    \n",
    "    # Process batches\n",
    "    for start in range(0, len(train_idx), batch_size):\n",
    "        batch_indices = train_idx[start:start + batch_size]\n",
    "        \n",
    "        # Create graph data for batch\n",
    "        nodes, edges, edge_weights, tensors = create_graph_data(batch_indices, direction_indices)\n",
    "        data_list = convert_to_torch_geometric(nodes, edges, edge_weights, tensors)\n",
    "        batch_data = Batch.from_data_list(data_list).to(device)\n",
    "        \n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_data.x, batch_data.edge_index, batch_data.edge_attr, batch_data.batch)\n",
    "        loss = model.weighted_mse_loss(pred, batch_data.y) * 1e6\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store batch results\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_pred_values.append(pred.detach().cpu().numpy())\n",
    "        epoch_gt_values.append(batch_data.y.cpu().numpy())\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Combine all batches\n",
    "    epoch_pred = np.concatenate(epoch_pred_values, axis=0)\n",
    "    epoch_gt = np.concatenate(epoch_gt_values, axis=0)\n",
    "    \n",
    "    # Print epoch results\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    print(\"\\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Evaluate epoch performance\n",
    "    if epoch % 1 == 0:\n",
    "        _ = evaluate_model(epoch_pred, epoch_gt, epoch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Final Evaluation...\n",
      "\n",
      "Final Test Results:\n",
      "Average test loss: 0.412770\n",
      "\n",
      "Epoch -1 Statistics:\n",
      "\n",
      "Relative Errors per component:\n",
      "Component 0: 2.87e+02\n",
      "Component 1: 1.56e+03\n",
      "Component 2: 3.06e+02\n",
      "Component 3: 1.24e+03\n",
      "Component 4: 5.75e+02\n",
      "Component 5: 2.95e+02\n",
      "Mean Diffusivity Error: 2.94e+02\n",
      "\n",
      "Prediction Statistics:\n",
      "Component 0: mean=9.82e-04, std=5.25e-04\n",
      "Component 1: mean=1.02e-04, std=1.01e-04\n",
      "Component 2: mean=1.02e-03, std=5.33e-04\n",
      "Component 3: mean=3.12e-05, std=1.00e-04\n",
      "Component 4: mean=-9.88e-06, std=1.08e-04\n",
      "Component 5: mean=1.00e-03, std=5.43e-04\n",
      "\n",
      "Ground Truth Statistics:\n",
      "Component 0: mean=9.82e-04, std=5.44e-04\n",
      "Component 1: mean=-5.51e-07, std=1.13e-04\n",
      "Component 2: mean=1.02e-03, std=5.49e-04\n",
      "Component 3: mean=2.80e-06, std=1.15e-04\n",
      "Component 4: mean=-2.30e-05, std=1.20e-04\n",
      "Component 5: mean=9.99e-04, std=5.54e-04\n",
      "\n",
      "Example predictions vs ground truth (randomly sampled):\n",
      "\n",
      "Sample 1:\n",
      "Predicted: [1.0977978e-03 8.6188316e-05 1.1421236e-03 1.8950552e-05 2.9838644e-05\n",
      " 1.1174589e-03]\n",
      "Actual:    [ 1.0716234e-03 -2.4549887e-05  1.1106026e-03 -3.4844775e-06\n",
      "  3.6027259e-06  1.1758426e-03]\n",
      "\n",
      "Sample 2:\n",
      "Predicted: [ 7.6934975e-04  1.6640872e-05  8.0894650e-04 -2.5555491e-05\n",
      " -5.4259785e-05  7.8099198e-04]\n",
      "Actual:    [ 7.5250946e-04 -6.3910382e-05  8.0585573e-04 -2.0171694e-05\n",
      " -7.1151968e-05  7.1980630e-04]\n",
      "\n",
      "Sample 3:\n",
      "Predicted: [ 7.5836433e-04  3.3718348e-04  7.9554872e-04  1.2957305e-04\n",
      " -1.8921681e-05  7.6885585e-04]\n",
      "Actual:    [ 6.8011152e-04  1.4491467e-04  7.5622415e-04  4.4617515e-05\n",
      " -5.3896780e-05  8.7621843e-04]\n",
      "\n",
      "Sample 4:\n",
      "Predicted: [9.7734854e-04 1.4797598e-04 1.0184107e-03 2.1692365e-05 7.4876472e-05\n",
      " 9.9329103e-04]\n",
      "Actual:    [ 9.0021535e-04  1.5819940e-05  9.9309662e-04 -4.8940619e-05\n",
      "  6.5985412e-05  9.3422318e-04]\n",
      "\n",
      "Sample 5:\n",
      "Predicted: [ 1.6041871e-03  1.3334304e-04  1.6549763e-03 -1.6666949e-05\n",
      "  7.9488382e-06  1.6389193e-03]\n",
      "Actual:    [ 1.5375983e-03 -1.1262277e-05  1.6367475e-03 -5.3269952e-05\n",
      " -3.1592223e-05  1.6358052e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting Final Evaluation...\")\n",
    "model.eval()\n",
    "test_losses = []\n",
    "test_preds = []\n",
    "test_gts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process all batches, including last partial batch\n",
    "    for start in range(0, len(test_idx), batch_size):\n",
    "        end = min(start + batch_size, len(test_idx))\n",
    "        batch_indices = test_idx[start:end]\n",
    "        \n",
    "        # Create and process batch\n",
    "        nodes, edges, edge_weights, tensors = create_graph_data(\n",
    "            batch_indices, direction_indices\n",
    "        )\n",
    "        data_list = convert_to_torch_geometric(nodes, edges, edge_weights, tensors)\n",
    "        batch_data = Batch.from_data_list(data_list).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = model(batch_data.x, batch_data.edge_index, batch_data.edge_attr, batch_data.batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        test_loss = model.weighted_mse_loss(pred, batch_data.y) * 1e6\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        # Store predictions and ground truth\n",
    "        test_preds.append(pred.cpu().numpy())\n",
    "        test_gts.append(batch_data.y.cpu().numpy())\n",
    "\n",
    "# Combine results\n",
    "all_test_preds = np.concatenate(test_preds, axis=0)\n",
    "all_test_gts = np.concatenate(test_gts, axis=0)\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"Average test loss: {avg_test_loss:.6f}\")\n",
    "\n",
    "# Use same evaluation function as training\n",
    "metrics = evaluate_model(all_test_preds, all_test_gts, epoch=-2)\n",
    "\n",
    "# Print random examples from full test set\n",
    "print(\"\\nExample predictions vs ground truth (randomly sampled):\")\n",
    "n_examples = 5\n",
    "rand_indices = np.random.choice(len(all_test_preds), n_examples, replace=False)\n",
    "for i, idx in enumerate(rand_indices):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Predicted: {all_test_preds[idx]}\")\n",
    "    print(f\"Actual:    {all_test_gts[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Original Ground Truth Tensor Analysis:\")\n",
    "# print(\"\\nComponent-wise statistics:\")\n",
    "# for i in range(6):\n",
    "#     comp = ground_truth_tensors[:, i]\n",
    "#     print(f\"\\nComponent {i}:\")\n",
    "#     print(f\"Min: {np.min(comp):.2e}\")\n",
    "#     print(f\"Max: {np.max(comp):.2e}\")\n",
    "#     print(f\"Mean: {np.mean(comp):.2e}\")\n",
    "#     print(f\"Std: {np.std(comp):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading ground truth data 2...\")\n",
    "# gt_data_v2 = np.load('ground_truth_v2.npz')\n",
    "# ground_truth_tensors_v2 = gt_data_v2['tensors']\n",
    "# print(f\"Ground truth tensors shape: {ground_truth_tensors_v2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"V2 Original Ground Truth Tensor Analysis:\")\n",
    "# print(\"\\nComponent-wise statistics:\")\n",
    "# for i in range(6):\n",
    "#     comp = ground_truth_tensors_v2[:, i]\n",
    "#     print(f\"\\nComponent {i}:\")\n",
    "#     print(f\"Min: {np.min(comp):.2e}\")\n",
    "#     print(f\"Max: {np.max(comp):.2e}\")\n",
    "#     print(f\"Mean: {np.mean(comp):.2e}\")\n",
    "#     print(f\"Std: {np.std(comp):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
