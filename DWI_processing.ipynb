{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"100206\"\n",
    "subject_path = f\"diffusion_data/{subject_id}/T1w/Diffusion\"\n",
    "dwi_img = nib.load(f'{subject_path}/data.nii.gz')\n",
    "mask_img = nib.load(f'{subject_path}/nodif_brain_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for processing\n",
    "dwi_data = dwi_img.get_fdata()\n",
    "mask = mask_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWI data shape: (145, 174, 145, 288)\n",
      "Mask shape: (145, 174, 145)\n"
     ]
    }
   ],
   "source": [
    "print(f\"DWI data shape: {dwi_data.shape}\")  # Should be (X, Y, Z, num_volumes)\n",
    "print(f\"Mask shape: {mask.shape}\")          # Should be (X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading gradient information...\n",
      "Number of gradient directions: 288\n",
      "bvals shape: (288,)\n",
      "bvecs shape: (288, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load gradient information (bvals and bvecs)\n",
    "print(\"\\nLoading gradient information...\")\n",
    "bvals, bvecs = read_bvals_bvecs(f'{subject_path}/bvals', \n",
    "                               f'{subject_path}/bvecs')\n",
    "print(f\"Number of gradient directions: {len(bvals)}\")\n",
    "print(f\"bvals shape: {bvals.shape}\")     # Should match number of volumes\n",
    "print(f\"bvecs shape: {bvecs.shape}\")     # Should be (num_volumes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gradient table for DIPY\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of B0 volumes: 18\n",
      "B0 data shape: (145, 174, 145, 18)\n",
      "Average B0 shape: (145, 174, 145)\n"
     ]
    }
   ],
   "source": [
    "# Identify and extract B0 (non-diffusion weighted) volumes\n",
    "b0_mask = gtab.b0s_mask\n",
    "b0_data = dwi_data[..., b0_mask]\n",
    "print(f\"\\nNumber of B0 volumes: {np.sum(b0_mask)}\")\n",
    "print(f\"B0 data shape: {b0_data.shape}\")\n",
    "# Average B0 volumes to get a single reference image\n",
    "b0_avg = np.mean(b0_data, axis=-1)\n",
    "print(f\"Average B0 shape: {b0_avg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of DWI volumes: 270\n",
      "DWI volumes shape: (145, 174, 145, 270)\n"
     ]
    }
   ],
   "source": [
    "# Extract and normalize diffusion weighted volumes\n",
    "dwi_mask = ~b0_mask  # Mask for diffusion weighted volumes\n",
    "dwi_vols = dwi_data[..., dwi_mask]\n",
    "print(f\"\\nNumber of DWI volumes: {np.sum(dwi_mask)}\")\n",
    "print(f\"DWI volumes shape: {dwi_vols.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized DWI shape: (145, 174, 145, 270)\n"
     ]
    }
   ],
   "source": [
    "# Normalize DWI volumes by B0 (avoid division by zero with small epsilon)\n",
    "dwi_norm = dwi_vols / (b0_avg[..., None] + 1e-6)\n",
    "print(f\"Normalized DWI shape: {dwi_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of valid voxels in mask: 936256\n"
     ]
    }
   ],
   "source": [
    "# Find valid voxels using the brain mask\n",
    "valid_idx = np.where(mask > 0)\n",
    "print(f\"\\nNumber of valid voxels in mask: {len(valid_idx[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random voxels for training\n",
    "n_samples = 1000  # Adjust this number based on your needs\n",
    "sample_idx = np.random.choice(len(valid_idx[0]), \n",
    "                            min(n_samples, len(valid_idx[0])), \n",
    "                            replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (signal intensities) from sampled voxels\n",
    "features = []\n",
    "for idx in sample_idx:\n",
    "    x, y, z = valid_idx[0][idx], valid_idx[1][idx], valid_idx[2][idx]\n",
    "    features.append(dwi_norm[x, y, z, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "gradient_directions = bvecs[dwi_mask]  # Only keep directions for DWI volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shapes:\n",
      "Features shape: (1000, 270)\n",
      "Gradient directions shape: (270, 3)\n",
      "\n",
      "Sanity checks:\n",
      "Max normalized value: 2.5465984403372413\n",
      "Min normalized value: 0.0\n",
      "Gradient directions magnitude close to 1: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal data shapes:\")\n",
    "print(f\"Features shape: {features.shape}\")           # Should be (n_samples, n_directions)\n",
    "print(f\"Gradient directions shape: {gradient_directions.shape}\")  # Should be (n_directions, 3)\n",
    "\n",
    "# Basic sanity checks\n",
    "print(\"\\nSanity checks:\")\n",
    "print(f\"Max normalized value: {np.max(features)}\")\n",
    "print(f\"Min normalized value: {np.min(features)}\")\n",
    "print(f\"Gradient directions magnitude close to 1: {np.allclose(np.linalg.norm(gradient_directions, axis=1), 1, atol=1e-3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graphs for 1000 samples using 41 directions each\n",
      "\n",
      "First sample stats:\n",
      "1000\n",
      "Number of nodes: 41\n",
      "Node feature shape: (41, 4)\n",
      "Number of edges: 398\n",
      "\n",
      "First 3 nodes of first sample:\n",
      "Format: [x, y, z, signal]\n",
      "[[ 0.336122    0.588045    0.73568     0.450987  ]\n",
      " [ 0.412554    0.523576   -0.74543     0.25648246]\n",
      " [ 0.953234    0.045028    0.298861    0.39993699]]\n",
      "\n",
      "First 3 edges of first sample:\n",
      "Edge 0: 0 -> 13, Angle: 35.25 degrees\n",
      "Edge 1: 0 -> 15, Angle: 37.25 degrees\n",
      "Edge 2: 0 -> 17, Angle: 18.42 degrees\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of directions to use for sparse estimation\n",
    "n_directions = 41  # We can adjust this number\n",
    "\n",
    "# Function to create graph data for a batch of samples\n",
    "def create_graph_data(features, gradient_directions, n_sparse_directions, threshold_angle):\n",
    "    \"\"\"\n",
    "    Creates graph data using a sparse subset of directions for each sample\n",
    "    \"\"\"\n",
    "    n_samples = features.shape[0]\n",
    "    print(f\"Creating graphs for {n_samples} samples using {n_sparse_directions} directions each\")\n",
    "    \n",
    "    # Lists to store batch data\n",
    "    batch_nodes = []\n",
    "    batch_edges = []\n",
    "    batch_indices = []  # Store which directions were used\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Randomly select directions\n",
    "        selected_idx = np.random.choice(\n",
    "            len(gradient_directions), \n",
    "            size=n_sparse_directions, \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Get selected directions and their features\n",
    "        directions = gradient_directions[selected_idx]\n",
    "        signals = features[i, selected_idx]\n",
    "        \n",
    "        # Create node features [x,y,z,signal]\n",
    "        nodes = np.column_stack([directions, signals])\n",
    "        \n",
    "        # Create edges between similar directions using cosine similarity\n",
    "        cos_sim = np.dot(directions, directions.T)\n",
    "        # Convert to angles in degrees\n",
    "        angles = np.arccos(np.clip(cos_sim, -1.0, 1.0)) * 180 / np.pi\n",
    "        # Find edges where angle < threshold (45 degrees)\n",
    "        src, dst = np.where(angles < threshold_angle)\n",
    "        # Remove self-loops\n",
    "        mask = src != dst\n",
    "        edges = np.column_stack([src[mask], dst[mask]])\n",
    "        \n",
    "        batch_nodes.append(nodes)\n",
    "        batch_edges.append(edges)\n",
    "        batch_indices.append(selected_idx)\n",
    "        \n",
    "    return batch_nodes, batch_edges, batch_indices\n",
    "\n",
    "# Create graph data for our samples\n",
    "nodes, edges, indices = create_graph_data(features, gradient_directions, n_directions, 45)\n",
    "\n",
    "# Print info for verification\n",
    "print(f\"\\nFirst sample stats:\")\n",
    "print(len(nodes))\n",
    "print(f\"Number of nodes: {nodes[0].shape[0]}\")\n",
    "print(f\"Node feature shape: {nodes[0].shape}\")\n",
    "print(f\"Number of edges: {edges[0].shape[0]}\")\n",
    "\n",
    "# Print example node features and edges\n",
    "print(\"\\nFirst 3 nodes of first sample:\")\n",
    "print(\"Format: [x, y, z, signal]\")\n",
    "print(nodes[0][:3])\n",
    "\n",
    "print(\"\\nFirst 3 edges of first sample:\")\n",
    "for i in range(min(3, len(edges[0]))):\n",
    "    edge = edges[0][i]\n",
    "    src, dst = edge\n",
    "    angle = np.arccos(np.clip(\n",
    "        np.dot(nodes[0][src, :3], nodes[0][dst, :3]), \n",
    "        -1.0, 1.0\n",
    "    )) * 180 / np.pi\n",
    "    print(f\"Edge {i}: {src} -> {dst}, Angle: {angle:.2f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
