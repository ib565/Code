{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"100206\"\n",
    "subject_path = f\"diffusion_data/{subject_id}/T1w/Diffusion\"\n",
    "dwi_img = nib.load(f'{subject_path}/data.nii.gz')\n",
    "mask_img = nib.load(f'{subject_path}/nodif_brain_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for processing\n",
    "dwi_data = dwi_img.get_fdata()\n",
    "mask = mask_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWI data shape: (145, 174, 145, 288)\n",
      "Mask shape: (145, 174, 145)\n"
     ]
    }
   ],
   "source": [
    "print(f\"DWI data shape: {dwi_data.shape}\")  # Should be (X, Y, Z, num_volumes)\n",
    "print(f\"Mask shape: {mask.shape}\")          # Should be (X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading gradient information...\n",
      "Number of gradient directions: 288\n",
      "bvals shape: (288,)\n",
      "bvecs shape: (288, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load gradient information (bvals and bvecs)\n",
    "print(\"\\nLoading gradient information...\")\n",
    "bvals, bvecs = read_bvals_bvecs(f'{subject_path}/bvals', \n",
    "                               f'{subject_path}/bvecs')\n",
    "print(f\"Number of gradient directions: {len(bvals)}\")\n",
    "print(f\"bvals shape: {bvals.shape}\")     # Should match number of volumes\n",
    "print(f\"bvecs shape: {bvecs.shape}\")     # Should be (num_volumes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gradient table for DIPY\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of b=1000 directions: 90\n",
      "\n",
      "Number of B0 volumes: 18\n",
      "B0 data shape: (145, 174, 145, 18)\n",
      "Average B0 shape: (145, 174, 145)\n"
     ]
    }
   ],
   "source": [
    "# Identify and extract B0 (non-diffusion weighted) volumes\n",
    "b0_mask = gtab.b0s_mask\n",
    "b1000_mask = (bvals >= 990) & (bvals <= 1010)\n",
    "print(f\"Number of b=1000 directions: {np.sum(b1000_mask)}\")\n",
    "b0_data = dwi_data[..., b0_mask]\n",
    "print(f\"\\nNumber of B0 volumes: {np.sum(b0_mask)}\")\n",
    "print(f\"B0 data shape: {b0_data.shape}\")\n",
    "# Average B0 volumes to get a single reference image\n",
    "b0_avg = np.mean(b0_data, axis=-1)\n",
    "print(f\"Average B0 shape: {b0_avg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of DWI volumes: 270\n",
      "DWI volumes shape: (145, 174, 145, 90)\n"
     ]
    }
   ],
   "source": [
    "# Extract and normalize diffusion weighted volumes\n",
    "dwi_mask = ~b0_mask  # Mask for diffusion weighted volumes\n",
    "dwi_vols = dwi_data[..., b1000_mask]\n",
    "print(f\"\\nNumber of DWI volumes: {np.sum(dwi_mask)}\")\n",
    "print(f\"DWI volumes shape: {dwi_vols.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize DWI volumes by B0 (avoid division by zero with small epsilon)\n",
    "# dwi_norm = dwi_vols / (b0_avg[..., None] + 1e-6)\n",
    "# print(f\"Normalized DWI shape: {dwi_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of valid voxels in mask: 936256\n"
     ]
    }
   ],
   "source": [
    "# Find valid voxels using the brain mask\n",
    "valid_idx = np.where(mask > 0)\n",
    "print(f\"\\nNumber of valid voxels in mask: {len(valid_idx[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random voxels for training\n",
    "n_samples = 50000  # Adjust this number based on your needs\n",
    "sample_idx = np.random.choice(len(valid_idx[0]), \n",
    "                            min(n_samples, len(valid_idx[0])), \n",
    "                            replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (signal intensities) from sampled voxels\n",
    "features = []\n",
    "for idx in sample_idx:\n",
    "    x, y, z = valid_idx[0][idx], valid_idx[1][idx], valid_idx[2][idx]\n",
    "    features.append(dwi_vols[x, y, z, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "gradient_directions = bvecs[b1000_mask]  # Only keep directions for DWI volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_max = np.max(features)\n",
    "features = features / feature_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shapes:\n",
      "Features shape: (50000, 90)\n",
      "Gradient directions shape: (90, 3)\n",
      "\n",
      "Sanity checks:\n",
      "Max normalized value: 1.0\n",
      "Min normalized value: 0.0\n",
      "Gradient directions magnitude close to 1: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal data shapes:\")\n",
    "print(f\"Features shape: {features.shape}\")           # Should be (n_samples, n_directions)\n",
    "print(f\"Gradient directions shape: {gradient_directions.shape}\")  # Should be (n_directions, 3)\n",
    "\n",
    "# Basic sanity checks\n",
    "print(\"\\nSanity checks:\")\n",
    "print(f\"Max normalized value: {np.max(features)}\")\n",
    "print(f\"Min normalized value: {np.min(features)}\")\n",
    "print(f\"Gradient directions magnitude close to 1: {np.allclose(np.linalg.norm(gradient_directions, axis=1), 1, atol=1e-3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth data...\n",
      "Ground truth tensors shape: (936256, 6)\n",
      "Valid coordinates shape: (936256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading ground truth data...\")\n",
    "gt_data = np.load('ground_truth.npz')\n",
    "ground_truth_tensors = gt_data['tensors']\n",
    "# ground_truth_tensors = torch.tensor(ground_truth_tensors, dtype=torch.float32)\n",
    "valid_coordinates = gt_data['coordinates']\n",
    "print(f\"Ground truth tensors shape: {ground_truth_tensors.shape}\")\n",
    "print(f\"Valid coordinates shape: {valid_coordinates.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of directions to use for sparse estimation\n",
    "n_directions = 21  # We can adjust this number\n",
    "\n",
    "def create_graph_data(features, gradient_directions, n_sparse_directions, batch_size = 32, threshold_angle = 45):\n",
    "    \"\"\"\n",
    "    Create graph data for a batch of samples, including ground tensor\n",
    "    \"\"\"\n",
    "    voxel_indices = np.random.choice(len(features), size = batch_size, replace = False)\n",
    "    # print(f\"Creating graphs for batch of {batch_size} voxels using {n_sparse_directions} directions each\")\n",
    "\n",
    "    batch_nodes = []\n",
    "    batch_edges = []\n",
    "    batch_tensors = []\n",
    "\n",
    "    for idx in voxel_indices:\n",
    "        # Randomly selected directions\n",
    "        selected_dir_idx = np.random.choice(len(gradient_directions), size=n_sparse_directions, replace=False)\n",
    "\n",
    "        directions = gradient_directions[selected_dir_idx]\n",
    "        signals = features[idx, selected_dir_idx]\n",
    "        \n",
    "        # Create node features: x, y, z, signal\n",
    "        nodes = np.column_stack([directions, signals])\n",
    "\n",
    "        # Create edges acc. to angle\n",
    "        directions_norm = directions / np.linalg.norm(directions, axis=1, keepdims=True)\n",
    "        cos_sim = np.dot(directions_norm, directions_norm.T)\n",
    "        angles = np.arccos(np.clip(cos_sim, -1.0, 1.0)) * 180/np.pi\n",
    "        \n",
    "        src, dst = np.where(angles < threshold_angle)\n",
    "        mask = src != dst\n",
    "        edges = np.column_stack([src[mask], dst[mask]])\n",
    "\n",
    "        batch_nodes.append(nodes)\n",
    "        batch_edges.append(edges)\n",
    "        batch_tensors.append(ground_truth_tensors[idx])\n",
    "        \n",
    "    return batch_nodes, batch_edges, batch_tensors\n",
    "\n",
    "# nodes, edges, gt_tensors = create_graph_data(features, gradient_directions, n_directions)\n",
    "# # Print info for verification\n",
    "# print(\"\\nBatch statistics:\")\n",
    "# print(f\"Number of samples in batch: {len(nodes)}\")\n",
    "# print(f\"Number of nodes per graph: {nodes[0].shape[0]}\")\n",
    "# print(f\"Node feature dimensionality: {nodes[0].shape[1]}\")\n",
    "# print(f\"Ground truth tensors: {len(gt_tensors)}\")\n",
    "\n",
    "# # Print example for first sample\n",
    "# print(\"\\nFirst sample in batch:\")\n",
    "# print(f\"Number of nodes: {nodes[0].shape[0]}\")\n",
    "# print(f\"Number of edges: {edges[0].shape[0]}\")\n",
    "# print(f\"Ground truth tensor components: {gt_tensors[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, Batch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionGNN(torch.nn.Module):\n",
    "    def __init__(self, node_features = 4, hidden_dim = 32):\n",
    "        super(DiffusionGNN, self).__init__()\n",
    "    \n",
    "        # Layers\n",
    "        self.conv1 = DiffusionConv(node_features, hidden_dim)\n",
    "        self.conv2 = DiffusionConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # MLP for tensor prediction\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 6),\n",
    "        )\n",
    "\n",
    "        self.scale_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        x: node_features\n",
    "        edge_index = graph conn info [2, num_edges]\n",
    "        batch: batch assignment for nodes\n",
    "        \"\"\"\n",
    "        # pass through gnn layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # combine node features for graph\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # raw predictions\n",
    "        out = self.mlp(x)\n",
    "\n",
    "        scale = self.scale_net(x)\n",
    "        \n",
    "        diag_idx = [0, 2, 5]\n",
    "        diag = out[:, diag_idx]\n",
    "\n",
    "        offdiag_idx = [1, 3, 4]\n",
    "        offdiag = out[:, offdiag_idx]\n",
    "\n",
    "        # Diagonal elements: [0,1] using sigmoid\n",
    "        diag = torch.sigmoid(diag)\n",
    "        \n",
    "        # Off-diagonal elements: [-1,1] using tanh\n",
    "        offdiag = torch.tanh(offdiag)\n",
    "\n",
    "        out_reordered = torch.zeros_like(out)\n",
    "        out_reordered[:, diag_idx] = diag      # Put diagonal components in right places\n",
    "        out_reordered[:, offdiag_idx] = offdiag  # Put off-diagonal components in right places\n",
    "        \n",
    "        return out_reordered * scale\n",
    "\n",
    "\n",
    "class DiffusionConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DiffusionConv, self).__init__(aggr=\"mean\")\n",
    "        \n",
    "        # MLP to process messages\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * in_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"\n",
    "        x_i: features of target nodes\n",
    "        x_j: features of source nodes\n",
    "        Returns: messages to be aggregated\n",
    "        \"\"\"\n",
    "        tmp = torch.cat([x_i, x_j], dim = 1)\n",
    "        return self.mlp(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_torch_geometric(nodes, edges, tensors):\n",
    "    data_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        # Ensure tensor is properly shaped [6] not flattened\n",
    "        tensor = tensors[i].reshape(-1, 6) if len(tensors[i].shape) == 1 else tensors[i]\n",
    "        \n",
    "        data = Data(\n",
    "            x=torch.FloatTensor(nodes[i]),          # [n_nodes, 4]\n",
    "            edge_index=torch.LongTensor(edges[i].T), # [2, n_edges]\n",
    "            y=torch.FloatTensor(tensor)             # [1, 6]\n",
    "        )\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 40000 voxels, testing with 10000 voxels\n"
     ]
    }
   ],
   "source": [
    "train_idx, test_idx = train_test_split(np.arange(len(features)), test_size=0.2, random_state=42)\n",
    "print(f\"Training with {len(train_idx)} voxels, testing with {len(test_idx)} voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n",
      "\n",
      "Epoch 1 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=1.70e-03, max=5.33e-03, mean=2.86e-03, std=7.98e-04\n",
      "Component 1: min=-4.29e-05, max=9.08e-05, mean=-1.24e-06, std=3.31e-05\n",
      "Component 2: min=1.81e-03, max=5.37e-03, mean=2.97e-03, std=7.84e-04\n",
      "Component 3: min=-1.25e-04, max=3.28e-04, mean=1.68e-05, std=9.89e-05\n",
      "Component 4: min=-1.46e-04, max=3.07e-04, mean=-1.55e-05, std=9.91e-05\n",
      "Component 5: min=1.02e-03, max=3.50e-03, mean=1.80e-03, std=5.41e-04\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 9.47e+00\n",
      "Component 1: 5.88e+00\n",
      "Component 2: 9.16e+00\n",
      "Component 3: 4.89e+01\n",
      "Component 4: 1.18e+01\n",
      "Component 5: 5.74e+00\n",
      "Epoch 1/10, Loss: 5272.858166\n",
      "\n",
      "Epoch 2 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=1.01e-04, max=5.71e-04, mean=2.62e-04, std=9.88e-05\n",
      "Component 1: min=-1.94e-05, max=4.24e-05, mean=-7.04e-07, std=1.33e-05\n",
      "Component 2: min=1.17e-04, max=6.11e-04, mean=2.89e-04, std=1.04e-04\n",
      "Component 3: min=-7.07e-05, max=1.11e-04, mean=-7.47e-06, std=3.88e-05\n",
      "Component 4: min=-8.91e-05, max=8.72e-05, mean=-3.21e-05, std=3.74e-05\n",
      "Component 5: min=8.52e-05, max=4.89e-04, mean=2.23e-04, std=8.49e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 4.19e-01\n",
      "Component 1: 2.00e+00\n",
      "Component 2: 3.82e-01\n",
      "Component 3: 1.85e+01\n",
      "Component 4: 6.35e+00\n",
      "Component 5: 4.12e-01\n",
      "Epoch 2/10, Loss: 0.417820\n",
      "\n",
      "Epoch 3 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=1.11e-04, max=4.85e-04, mean=2.63e-04, std=9.37e-05\n",
      "Component 1: min=-1.96e-05, max=2.60e-05, mean=-2.98e-06, std=1.12e-05\n",
      "Component 2: min=1.25e-04, max=5.12e-04, mean=2.84e-04, std=9.68e-05\n",
      "Component 3: min=-5.49e-05, max=8.20e-05, mean=-1.28e-06, std=3.38e-05\n",
      "Component 4: min=-8.26e-05, max=5.07e-05, mean=-2.98e-05, std=3.36e-05\n",
      "Component 5: min=1.10e-04, max=4.73e-04, mean=2.58e-04, std=9.08e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.48e-01\n",
      "Component 1: 1.54e+00\n",
      "Component 2: 3.36e-01\n",
      "Component 3: 5.50e+00\n",
      "Component 4: 5.63e+00\n",
      "Component 5: 3.41e-01\n",
      "Epoch 3/10, Loss: 0.011957\n",
      "\n",
      "Epoch 4 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=1.65e-04, max=5.09e-04, mean=2.84e-04, std=6.89e-05\n",
      "Component 1: min=-1.65e-05, max=2.55e-05, mean=-2.65e-06, std=8.51e-06\n",
      "Component 2: min=1.80e-04, max=5.30e-04, mean=3.03e-04, std=7.02e-05\n",
      "Component 3: min=-2.65e-05, max=8.69e-05, mean=1.22e-05, std=2.30e-05\n",
      "Component 4: min=-5.56e-05, max=6.05e-05, mean=-1.55e-05, std=2.38e-05\n",
      "Component 5: min=1.61e-04, max=4.93e-04, mean=2.77e-04, std=6.64e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.96e-01\n",
      "Component 1: 2.11e+00\n",
      "Component 2: 3.63e-01\n",
      "Component 3: 1.28e+01\n",
      "Component 4: 3.34e+00\n",
      "Component 5: 3.82e-01\n",
      "Epoch 4/10, Loss: 0.008503\n",
      "\n",
      "Epoch 5 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=2.08e-04, max=4.33e-04, mean=3.05e-04, std=5.35e-05\n",
      "Component 1: min=-1.26e-05, max=9.44e-06, mean=-1.36e-06, std=5.11e-06\n",
      "Component 2: min=2.24e-04, max=4.51e-04, mean=3.22e-04, std=5.41e-05\n",
      "Component 3: min=-1.36e-05, max=5.84e-05, mean=1.77e-05, std=1.78e-05\n",
      "Component 4: min=-4.10e-05, max=3.60e-05, mean=-8.92e-06, std=1.85e-05\n",
      "Component 5: min=2.03e-04, max=4.20e-04, mean=2.96e-04, std=5.16e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 4.08e-01\n",
      "Component 1: 1.19e+00\n",
      "Component 2: 3.78e-01\n",
      "Component 3: 7.69e+00\n",
      "Component 4: 2.83e+00\n",
      "Component 5: 3.62e-01\n",
      "Epoch 5/10, Loss: 0.006222\n",
      "\n",
      "Epoch 6 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=2.29e-04, max=4.57e-04, mean=3.05e-04, std=5.82e-05\n",
      "Component 1: min=-7.09e-06, max=1.69e-05, mean=1.59e-06, std=6.63e-06\n",
      "Component 2: min=2.44e-04, max=4.74e-04, mean=3.21e-04, std=5.87e-05\n",
      "Component 3: min=-1.15e-05, max=6.19e-05, mean=1.68e-05, std=1.82e-05\n",
      "Component 4: min=-3.49e-05, max=4.19e-05, mean=-9.82e-06, std=2.00e-05\n",
      "Component 5: min=2.22e-04, max=4.41e-04, mean=2.96e-04, std=5.60e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.59e-01\n",
      "Component 1: 1.29e+00\n",
      "Component 2: 3.37e-01\n",
      "Component 3: 1.20e+01\n",
      "Component 4: 2.90e+00\n",
      "Component 5: 3.51e-01\n",
      "Epoch 6/10, Loss: 0.005862\n",
      "\n",
      "Epoch 7 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=2.38e-04, max=4.54e-04, mean=3.02e-04, std=5.15e-05\n",
      "Component 1: min=-1.30e-05, max=8.34e-06, mean=-3.34e-06, std=5.21e-06\n",
      "Component 2: min=2.54e-04, max=4.72e-04, mean=3.18e-04, std=5.20e-05\n",
      "Component 3: min=-4.44e-06, max=6.28e-05, mean=1.55e-05, std=1.61e-05\n",
      "Component 4: min=-3.33e-05, max=4.10e-05, mean=-1.08e-05, std=1.73e-05\n",
      "Component 5: min=2.31e-04, max=4.39e-04, mean=2.92e-04, std=4.95e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.61e-01\n",
      "Component 1: 1.60e+00\n",
      "Component 2: 3.18e-01\n",
      "Component 3: 2.00e+01\n",
      "Component 4: 2.34e+00\n",
      "Component 5: 3.21e-01\n",
      "Epoch 7/10, Loss: 0.005756\n",
      "\n",
      "Epoch 8 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=2.27e-04, max=3.94e-04, mean=2.90e-04, std=4.10e-05\n",
      "Component 1: min=-1.18e-05, max=5.86e-06, mean=-2.78e-06, std=4.49e-06\n",
      "Component 2: min=2.43e-04, max=4.12e-04, mean=3.06e-04, std=4.15e-05\n",
      "Component 3: min=-5.12e-06, max=4.55e-05, mean=1.29e-05, std=1.17e-05\n",
      "Component 4: min=-3.43e-05, max=1.71e-05, mean=-1.45e-05, std=1.24e-05\n",
      "Component 5: min=2.21e-04, max=3.82e-04, mean=2.81e-04, std=3.94e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.25e-01\n",
      "Component 1: 1.35e+00\n",
      "Component 2: 2.97e-01\n",
      "Component 3: 5.50e+00\n",
      "Component 4: 2.74e+00\n",
      "Component 5: 3.00e-01\n",
      "Epoch 8/10, Loss: 0.005826\n",
      "\n",
      "Epoch 9 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=2.49e-04, max=4.28e-04, mean=3.11e-04, std=5.56e-05\n",
      "Component 1: min=-1.28e-05, max=3.83e-06, mean=-3.90e-06, std=3.89e-06\n",
      "Component 2: min=2.65e-04, max=4.45e-04, mean=3.28e-04, std=5.61e-05\n",
      "Component 3: min=5.67e-06, max=5.20e-05, mean=1.90e-05, std=1.48e-05\n",
      "Component 4: min=-2.65e-05, max=2.89e-05, mean=-6.52e-06, std=1.53e-05\n",
      "Component 5: min=2.42e-04, max=4.14e-04, mean=3.02e-04, std=5.35e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.48e-01\n",
      "Component 1: 1.31e+00\n",
      "Component 2: 3.14e-01\n",
      "Component 3: 5.98e+00\n",
      "Component 4: 1.65e+00\n",
      "Component 5: 3.27e-01\n",
      "Epoch 9/10, Loss: 0.005748\n",
      "\n",
      "Epoch 10 Statistics:\n",
      "\n",
      "Predictions per component:\n",
      "Component 0: min=2.52e-04, max=4.07e-04, mean=2.92e-04, std=3.24e-05\n",
      "Component 1: min=-3.34e-07, max=1.55e-05, mean=4.52e-06, std=3.68e-06\n",
      "Component 2: min=2.68e-04, max=4.25e-04, mean=3.08e-04, std=3.27e-05\n",
      "Component 3: min=-6.92e-06, max=2.80e-05, mean=2.30e-06, std=6.90e-06\n",
      "Component 4: min=-3.27e-05, max=1.12e-05, mean=-2.23e-05, std=8.70e-06\n",
      "Component 5: min=2.45e-04, max=3.94e-04, mean=2.83e-04, std=3.12e-05\n",
      "\n",
      "Ground Truth per component:\n",
      "Component 0: min=1.62e-04, max=5.48e-04, mean=3.04e-04, std=9.81e-05\n",
      "Component 1: min=-4.25e-05, max=4.70e-05, mean=-1.67e-06, std=2.24e-05\n",
      "Component 2: min=1.67e-04, max=4.85e-04, mean=3.20e-04, std=9.22e-05\n",
      "Component 3: min=-1.88e-05, max=4.25e-05, mean=1.49e-05, std=1.61e-05\n",
      "Component 4: min=-6.59e-05, max=2.80e-05, mean=-1.14e-05, std=2.10e-05\n",
      "Component 5: min=1.43e-04, max=4.65e-04, mean=2.95e-04, std=8.37e-05\n",
      "\n",
      "Relative Error per component:\n",
      "Component 0: 3.22e-01\n",
      "Component 1: 1.24e+00\n",
      "Component 2: 3.05e-01\n",
      "Component 3: 1.95e+00\n",
      "Component 4: 3.02e+00\n",
      "Component 5: 3.11e-01\n",
      "Epoch 10/10, Loss: 0.005702\n",
      "\n",
      "Training Values Statistics:\n",
      "\n",
      "Component 0:\n",
      "Predictions:\n",
      "Min: 0.000101\n",
      "Max: 0.005333\n",
      "Mean: 0.000548\n",
      "1st percentile: 0.000126\n",
      "99th percentile: 0.003731\n",
      "Ground Truth:\n",
      "Min: 0.000162\n",
      "Max: 0.000548\n",
      "Mean: 0.000304\n",
      "1st percentile: 0.000162\n",
      "99th percentile: 0.000548\n",
      "\n",
      "Component 1:\n",
      "Predictions:\n",
      "Min: -0.000043\n",
      "Max: 0.000091\n",
      "Mean: -0.000001\n",
      "1st percentile: -0.000036\n",
      "99th percentile: 0.000041\n",
      "Ground Truth:\n",
      "Min: -0.000042\n",
      "Max: 0.000047\n",
      "Mean: -0.000002\n",
      "1st percentile: -0.000042\n",
      "99th percentile: 0.000047\n",
      "\n",
      "Component 2:\n",
      "Predictions:\n",
      "Min: 0.000117\n",
      "Max: 0.005372\n",
      "Mean: 0.000575\n",
      "1st percentile: 0.000143\n",
      "99th percentile: 0.003827\n",
      "Ground Truth:\n",
      "Min: 0.000167\n",
      "Max: 0.000485\n",
      "Mean: 0.000320\n",
      "1st percentile: 0.000167\n",
      "99th percentile: 0.000485\n",
      "\n",
      "Component 3:\n",
      "Predictions:\n",
      "Min: -0.000125\n",
      "Max: 0.000328\n",
      "Mean: 0.000010\n",
      "1st percentile: -0.000088\n",
      "99th percentile: 0.000124\n",
      "Ground Truth:\n",
      "Min: -0.000019\n",
      "Max: 0.000043\n",
      "Mean: 0.000015\n",
      "1st percentile: -0.000019\n",
      "99th percentile: 0.000043\n",
      "\n",
      "Component 4:\n",
      "Predictions:\n",
      "Min: -0.000146\n",
      "Max: 0.000307\n",
      "Mean: -0.000017\n",
      "1st percentile: -0.000118\n",
      "99th percentile: 0.000103\n",
      "Ground Truth:\n",
      "Min: -0.000066\n",
      "Max: 0.000028\n",
      "Mean: -0.000011\n",
      "1st percentile: -0.000066\n",
      "99th percentile: 0.000028\n",
      "\n",
      "Component 5:\n",
      "Predictions:\n",
      "Min: 0.000085\n",
      "Max: 0.003495\n",
      "Mean: 0.000430\n",
      "1st percentile: 0.000125\n",
      "99th percentile: 0.002379\n",
      "Ground Truth:\n",
      "Min: 0.000143\n",
      "Max: 0.000465\n",
      "Mean: 0.000295\n",
      "1st percentile: 0.000143\n",
      "99th percentile: 0.000465\n",
      "\n",
      "Final Evaluation...\n",
      "Test Loss: 0.004408\n",
      "Test Loss: 0.004920\n",
      "Test Loss: 0.004303\n",
      "Test Loss: 0.005091\n",
      "Test Loss: 0.006089\n",
      "Test Loss: 0.005487\n",
      "Test Loss: 0.005158\n",
      "Test Loss: 0.004647\n",
      "Test Loss: 0.005087\n",
      "Test Loss: 0.005505\n",
      "Test Loss: 0.007153\n",
      "Test Loss: 0.005300\n",
      "Test Loss: 0.004854\n",
      "Test Loss: 0.006908\n",
      "Test Loss: 0.005242\n",
      "Test Loss: 0.005771\n",
      "Test Loss: 0.005079\n",
      "Test Loss: 0.006001\n",
      "Test Loss: 0.005535\n",
      "Test Loss: 0.005264\n",
      "Test Loss: 0.005618\n",
      "Test Loss: 0.004811\n",
      "Test Loss: 0.005489\n",
      "Test Loss: 0.006884\n",
      "Test Loss: 0.005241\n",
      "Test Loss: 0.008140\n",
      "Test Loss: 0.010055\n",
      "Test Loss: 0.005631\n",
      "Test Loss: 0.006307\n",
      "Test Loss: 0.007524\n",
      "Test Loss: 0.007699\n",
      "Test Loss: 0.005292\n",
      "Test Loss: 0.004595\n",
      "Test Loss: 0.005158\n",
      "Test Loss: 0.005539\n",
      "Test Loss: 0.005798\n",
      "Test Loss: 0.004628\n",
      "Test Loss: 0.004923\n",
      "Test Loss: 0.004341\n",
      "Test Loss: 0.006395\n",
      "Test Loss: 0.006570\n",
      "Test Loss: 0.004838\n",
      "Test Loss: 0.005820\n",
      "Test Loss: 0.007384\n",
      "Test Loss: 0.004607\n",
      "Test Loss: 0.005433\n",
      "Test Loss: 0.005051\n",
      "Test Loss: 0.004291\n",
      "Test Loss: 0.004998\n",
      "Test Loss: 0.004657\n",
      "Test Loss: 0.006087\n",
      "Test Loss: 0.005554\n",
      "Test Loss: 0.004567\n",
      "Test Loss: 0.005192\n",
      "Test Loss: 0.008426\n",
      "Test Loss: 0.005575\n",
      "Test Loss: 0.005531\n",
      "Test Loss: 0.006983\n",
      "Test Loss: 0.005342\n",
      "Test Loss: 0.005730\n",
      "Test Loss: 0.004741\n",
      "Test Loss: 0.006213\n",
      "Test Loss: 0.005240\n",
      "Test Loss: 0.007994\n",
      "Test Loss: 0.004815\n",
      "Test Loss: 0.004582\n",
      "Test Loss: 0.004988\n",
      "Test Loss: 0.004302\n",
      "Test Loss: 0.004977\n",
      "Test Loss: 0.005250\n",
      "Test Loss: 0.006623\n",
      "Test Loss: 0.006220\n",
      "Test Loss: 0.005829\n",
      "Test Loss: 0.004956\n",
      "Test Loss: 0.006138\n",
      "Test Loss: 0.005452\n",
      "Test Loss: 0.007046\n",
      "Test Loss: 0.007422\n",
      "Test Loss: 0.005477\n",
      "Test Loss: 0.004559\n",
      "Test Loss: 0.004205\n",
      "Test Loss: 0.003627\n",
      "Test Loss: 0.004935\n",
      "Test Loss: 0.006410\n",
      "Test Loss: 0.008170\n",
      "Test Loss: 0.006078\n",
      "Test Loss: 0.005121\n",
      "Test Loss: 0.004754\n",
      "Test Loss: 0.005518\n",
      "Test Loss: 0.005309\n",
      "Test Loss: 0.004287\n",
      "Test Loss: 0.005476\n",
      "Test Loss: 0.007291\n",
      "Test Loss: 0.005560\n",
      "Test Loss: 0.005041\n",
      "Test Loss: 0.004847\n",
      "Test Loss: 0.006310\n",
      "Test Loss: 0.011120\n",
      "Test Loss: 0.005019\n",
      "Test Loss: 0.004509\n",
      "Test Loss: 0.005525\n",
      "Test Loss: 0.005779\n",
      "Test Loss: 0.006403\n",
      "Test Loss: 0.006221\n",
      "Test Loss: 0.008863\n",
      "Test Loss: 0.004113\n",
      "Test Loss: 0.004659\n",
      "Test Loss: 0.006430\n",
      "Test Loss: 0.006583\n",
      "Test Loss: 0.009410\n",
      "Test Loss: 0.005374\n",
      "Test Loss: 0.006274\n",
      "Test Loss: 0.005260\n",
      "Test Loss: 0.005324\n",
      "Test Loss: 0.004478\n",
      "Test Loss: 0.006425\n",
      "Test Loss: 0.005552\n",
      "Test Loss: 0.005660\n",
      "Test Loss: 0.006972\n",
      "Test Loss: 0.005566\n",
      "Test Loss: 0.005773\n",
      "Test Loss: 0.006231\n",
      "Test Loss: 0.005372\n",
      "Test Loss: 0.005882\n",
      "Test Loss: 0.006756\n",
      "Test Loss: 0.004546\n",
      "Test Loss: 0.005098\n",
      "Test Loss: 0.008398\n",
      "Test Loss: 0.006439\n",
      "Test Loss: 0.007607\n",
      "Test Loss: 0.005680\n",
      "Test Loss: 0.005144\n",
      "Test Loss: 0.004911\n",
      "Test Loss: 0.004737\n",
      "Test Loss: 0.006916\n",
      "Test Loss: 0.005295\n",
      "Test Loss: 0.005429\n",
      "Test Loss: 0.004523\n",
      "Test Loss: 0.005034\n",
      "Test Loss: 0.005278\n",
      "Test Loss: 0.005336\n",
      "Test Loss: 0.005476\n",
      "Test Loss: 0.005509\n",
      "Test Loss: 0.005767\n",
      "Test Loss: 0.005381\n",
      "Test Loss: 0.004253\n",
      "Test Loss: 0.005662\n",
      "Test Loss: 0.005264\n",
      "Test Loss: 0.005007\n",
      "Test Loss: 0.004926\n",
      "Test Loss: 0.005685\n",
      "Test Loss: 0.005372\n",
      "Test Loss: 0.005170\n",
      "Test Loss: 0.004641\n",
      "Test Loss: 0.007253\n",
      "Test Loss: 0.006522\n",
      "Test Loss: 0.004058\n",
      "Test Loss: 0.005687\n",
      "Test Loss: 0.005556\n",
      "Test Loss: 0.006453\n",
      "Test Loss: 0.005571\n",
      "Test Loss: 0.005044\n",
      "Test Loss: 0.006009\n",
      "Test Loss: 0.005521\n",
      "Test Loss: 0.009370\n",
      "Test Loss: 0.005934\n",
      "Test Loss: 0.005900\n",
      "Test Loss: 0.004820\n",
      "Test Loss: 0.008155\n",
      "Test Loss: 0.006977\n",
      "Test Loss: 0.006497\n",
      "Test Loss: 0.004508\n",
      "Test Loss: 0.004509\n",
      "Test Loss: 0.004326\n",
      "Test Loss: 0.005920\n",
      "Test Loss: 0.005190\n",
      "Test Loss: 0.005601\n",
      "Test Loss: 0.005776\n",
      "Test Loss: 0.005934\n",
      "Test Loss: 0.007345\n",
      "Test Loss: 0.004969\n",
      "Test Loss: 0.005892\n",
      "Test Loss: 0.005324\n",
      "Test Loss: 0.005974\n",
      "Test Loss: 0.004635\n",
      "Test Loss: 0.004637\n",
      "Test Loss: 0.004591\n",
      "Test Loss: 0.005396\n",
      "Test Loss: 0.005424\n",
      "Test Loss: 0.004974\n",
      "Test Loss: 0.004198\n",
      "Test Loss: 0.005615\n",
      "Test Loss: 0.004595\n",
      "Test Loss: 0.004646\n",
      "Test Loss: 0.005886\n",
      "Test Loss: 0.006658\n",
      "Test Loss: 0.004508\n",
      "Test Loss: 0.005251\n",
      "Test Loss: 0.004911\n",
      "Test Loss: 0.005220\n",
      "Test Loss: 0.006573\n",
      "Test Loss: 0.006257\n",
      "Test Loss: 0.005568\n",
      "Test Loss: 0.005403\n",
      "Test Loss: 0.005185\n",
      "Test Loss: 0.006090\n",
      "Test Loss: 0.005733\n",
      "Test Loss: 0.005469\n",
      "Test Loss: 0.004996\n",
      "Test Loss: 0.007371\n",
      "Test Loss: 0.003628\n",
      "Test Loss: 0.006345\n",
      "Test Loss: 0.010139\n",
      "Test Loss: 0.004492\n",
      "Test Loss: 0.004214\n",
      "Test Loss: 0.006592\n",
      "Test Loss: 0.006329\n",
      "Test Loss: 0.006047\n",
      "Test Loss: 0.004339\n",
      "Test Loss: 0.005340\n",
      "Test Loss: 0.005482\n",
      "Test Loss: 0.006896\n",
      "Test Loss: 0.004871\n",
      "Test Loss: 0.005302\n",
      "Test Loss: 0.006501\n",
      "Test Loss: 0.003738\n",
      "Test Loss: 0.005257\n",
      "Test Loss: 0.006035\n",
      "Test Loss: 0.007622\n",
      "Test Loss: 0.004661\n",
      "Test Loss: 0.004436\n",
      "Test Loss: 0.006658\n",
      "Test Loss: 0.004241\n",
      "Test Loss: 0.007231\n",
      "Test Loss: 0.005132\n",
      "Test Loss: 0.005416\n",
      "Test Loss: 0.007221\n",
      "Test Loss: 0.004974\n",
      "Test Loss: 0.005457\n",
      "Test Loss: 0.012781\n",
      "Test Loss: 0.005569\n",
      "Test Loss: 0.004852\n",
      "Test Loss: 0.005049\n",
      "Test Loss: 0.006064\n",
      "Test Loss: 0.005699\n",
      "Test Loss: 0.006283\n",
      "Test Loss: 0.004499\n",
      "Test Loss: 0.005003\n",
      "Test Loss: 0.005695\n",
      "Test Loss: 0.005362\n",
      "Test Loss: 0.005927\n",
      "Test Loss: 0.004635\n",
      "Test Loss: 0.006043\n",
      "Test Loss: 0.007380\n",
      "Test Loss: 0.004266\n",
      "Test Loss: 0.005531\n",
      "Test Loss: 0.004562\n",
      "Test Loss: 0.005957\n",
      "Test Loss: 0.005623\n",
      "Test Loss: 0.005186\n",
      "Test Loss: 0.005628\n",
      "Test Loss: 0.004679\n",
      "Test Loss: 0.005454\n",
      "Test Loss: 0.006234\n",
      "Test Loss: 0.004717\n",
      "Test Loss: 0.004190\n",
      "Test Loss: 0.008128\n",
      "Test Loss: 0.004840\n",
      "Test Loss: 0.004293\n",
      "Test Loss: 0.005868\n",
      "Test Loss: 0.006816\n",
      "Test Loss: 0.005084\n",
      "Test Loss: 0.007933\n",
      "Test Loss: 0.004805\n",
      "Test Loss: 0.005762\n",
      "Test Loss: 0.005573\n",
      "Test Loss: 0.005183\n",
      "Test Loss: 0.005935\n",
      "Test Loss: 0.004883\n",
      "Test Loss: 0.004899\n",
      "Test Loss: 0.005888\n",
      "Test Loss: 0.005689\n",
      "Test Loss: 0.007745\n",
      "Test Loss: 0.005236\n",
      "Test Loss: 0.006574\n",
      "Test Loss: 0.008260\n",
      "Test Loss: 0.004990\n",
      "Test Loss: 0.004774\n",
      "Test Loss: 0.007169\n",
      "Test Loss: 0.005655\n",
      "Test Loss: 0.005427\n",
      "Test Loss: 0.004539\n",
      "Test Loss: 0.005047\n",
      "Test Loss: 0.004876\n",
      "Test Loss: 0.005764\n",
      "Test Loss: 0.005269\n",
      "Test Loss: 0.005679\n",
      "Test Loss: 0.006151\n",
      "Test Loss: 0.005955\n",
      "Test Loss: 0.005124\n",
      "Test Loss: 0.009362\n",
      "Test Loss: 0.006807\n",
      "Test Loss: 0.005086\n",
      "Test Loss: 0.005509\n",
      "Test Loss: 0.003971\n",
      "Test Loss: 0.006194\n",
      "Test Loss: 0.007655\n",
      "Test Loss: 0.005147\n",
      "Test Loss: 0.005511\n",
      "Test Loss: 0.005400\n",
      "Test Loss: 0.004803\n",
      "Test Loss: 0.005461\n",
      "Test Loss: 0.005207\n",
      "Final average test loss: 0.005731\n",
      "\n",
      "Example predictions vs ground truth:\n",
      "\n",
      "Sample 1:\n",
      "Predicted: [ 2.7714286e-04  6.0402376e-06  2.9324670e-04  4.9761338e-06\n",
      " -1.6707198e-05  2.6910787e-04]\n",
      "Actual: [ 2.5550034e-04  6.5357231e-06  2.6182135e-04  2.3852454e-05\n",
      " -2.1309179e-05  2.4634704e-04]\n",
      "\n",
      "Sample 2:\n",
      "Predicted: [ 3.11897456e-04  7.76806428e-06  3.28845432e-04  1.50731885e-05\n",
      " -7.05263892e-06  3.02926142e-04]\n",
      "Actual: [3.5793151e-04 2.0427428e-06 3.3458660e-04 3.7328908e-05 6.4760961e-06\n",
      " 3.2365034e-04]\n",
      "\n",
      "Sample 3:\n",
      "Predicted: [ 2.9796947e-04  5.9849885e-06  3.1452571e-04  8.0449872e-06\n",
      " -1.5898817e-05  2.8917968e-04]\n",
      "Actual: [ 2.5598684e-04 -3.0749234e-05  3.0733991e-04  1.3824460e-05\n",
      " -1.9538650e-06  2.7666904e-04]\n",
      "\n",
      "Sample 4:\n",
      "Predicted: [4.0533545e-04 1.6398424e-05 4.2245488e-04 3.1807693e-05 8.2952001e-06\n",
      " 3.9227962e-04]\n",
      "Actual: [ 4.3640722e-04 -1.7859384e-05  4.4100621e-04  4.2521631e-05\n",
      " -1.5239211e-05  4.1429815e-04]\n",
      "\n",
      "Sample 5:\n",
      "Predicted: [ 2.9763934e-04  1.1491134e-05  3.1418513e-04  7.7139475e-06\n",
      " -1.1096877e-05  2.8890310e-04]\n",
      "Actual: [ 3.4041627e-04  3.0247827e-05  4.0526260e-04  2.0505873e-05\n",
      " -2.3044380e-05  3.2322097e-04]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DiffusionGNN(node_features=4, hidden_dim=32).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "all_pred_values = [[] for _ in range(6)]\n",
    "all_gt_values = [[] for _ in range(6)]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    # Shuffle training indices\n",
    "    np.random.shuffle(train_idx)\n",
    "    \n",
    "    # Process batches\n",
    "    for start in range(0, len(train_idx), batch_size):\n",
    "        batch_idx = train_idx[start:start + batch_size]\n",
    "        # Create graphs for batch\n",
    "        nodes, edges, tensors = create_graph_data(\n",
    "            features[batch_idx], gradient_directions, 21\n",
    "        )\n",
    "        \n",
    "        # Convert to PyG and process\n",
    "        data_list = convert_to_torch_geometric(nodes, edges, tensors)\n",
    "        batch_data = Batch.from_data_list(data_list).to(device)\n",
    "        \n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n",
    "        loss = F.mse_loss(pred, batch_data.y) * 1e6\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    for i in range(6):\n",
    "        all_pred_values[i].extend(pred[:, i].detach().cpu().numpy())\n",
    "        all_gt_values[i].extend(batch_data.y[:, i].cpu().numpy())\n",
    "\n",
    "    if epoch % 1 == 0:  # Print every epoch\n",
    "        # Move entire tensors to CPU once\n",
    "        pred_cpu = pred.detach().cpu().numpy()\n",
    "        gt_cpu = batch_data.y.cpu().numpy()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Statistics:\")\n",
    "        # Component-wise prediction stats\n",
    "        print(\"\\nPredictions per component:\")\n",
    "        for i in range(6):\n",
    "            print(f\"Component {i}: min={np.min(pred_cpu[:,i]):.2e}, \"\n",
    "                f\"max={np.max(pred_cpu[:,i]):.2e}, \"\n",
    "                f\"mean={np.mean(pred_cpu[:,i]):.2e}, \"\n",
    "                f\"std={np.std(pred_cpu[:,i]):.2e}\")\n",
    "        \n",
    "        # Component-wise ground truth stats\n",
    "        print(\"\\nGround Truth per component:\")\n",
    "        for i in range(6):\n",
    "            print(f\"Component {i}: min={np.min(gt_cpu[:,i]):.2e}, \"\n",
    "                f\"max={np.max(gt_cpu[:,i]):.2e}, \"\n",
    "                f\"mean={np.mean(gt_cpu[:,i]):.2e}, \"\n",
    "                f\"std={np.std(gt_cpu[:,i]):.2e}\")\n",
    "        \n",
    "        # Relative error per component\n",
    "        print(\"\\nRelative Error per component:\")\n",
    "        for i in range(6):\n",
    "            rel_error = np.mean(np.abs(pred_cpu[:,i] - gt_cpu[:,i]) / \n",
    "                            (np.abs(gt_cpu[:,i]) + 1e-10))\n",
    "            print(f\"Component {i}: {rel_error:.2e}\")\n",
    "    \n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "\n",
    "print(\"\\nTraining Values Statistics:\")\n",
    "for i in range(6):\n",
    "    pred_vals = np.array(all_pred_values[i])\n",
    "    gt_vals = np.array(all_gt_values[i])\n",
    "    print(f\"\\nComponent {i}:\")\n",
    "    print(\"Predictions:\")\n",
    "    print(f\"Min: {np.min(pred_vals):.6f}\")\n",
    "    print(f\"Max: {np.max(pred_vals):.6f}\")\n",
    "    print(f\"Mean: {np.mean(pred_vals):.6f}\")\n",
    "    print(f\"1st percentile: {np.percentile(pred_vals, 1):.6f}\")\n",
    "    print(f\"99th percentile: {np.percentile(pred_vals, 99):.6f}\")\n",
    "    print(\"Ground Truth:\")\n",
    "    print(f\"Min: {np.min(gt_vals):.6f}\")\n",
    "    print(f\"Max: {np.max(gt_vals):.6f}\")\n",
    "    print(f\"Mean: {np.mean(gt_vals):.6f}\")\n",
    "    print(f\"1st percentile: {np.percentile(gt_vals, 1):.6f}\")\n",
    "    print(f\"99th percentile: {np.percentile(gt_vals, 99):.6f}\")\n",
    "\n",
    "# Final evaluation on full test set\n",
    "print(\"\\nFinal Evaluation...\")\n",
    "model.eval()\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for start in range(0, len(test_idx), batch_size):\n",
    "        if len(features[batch_idx]) < 32:\n",
    "            continue\n",
    "        nodes, edges, tensors = create_graph_data(\n",
    "            features[batch_idx], gradient_directions, 21\n",
    "        )\n",
    "        data_list = convert_to_torch_geometric(nodes, edges, tensors)\n",
    "        batch_data = Batch.from_data_list(data_list).to(device)\n",
    "        \n",
    "        pred = model(batch_data.x, batch_data.edge_index, batch_data.batch)\n",
    "        pred = pred.cpu()\n",
    "        batch_data = batch_data.cpu()\n",
    "        test_loss = F.mse_loss(pred, batch_data.y) * 1e6\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "print(f\"Final average test loss: {avg_test_loss:.6f}\")\n",
    "\n",
    "# Print some example predictions vs ground truth\n",
    "print(\"\\nExample predictions vs ground truth:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Predicted: {pred[i].numpy()}\")\n",
    "    print(f\"Actual: {batch_data.y[i].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Ground Truth Tensor Analysis:\n",
      "\n",
      "Component-wise statistics:\n",
      "\n",
      "Component 0:\n",
      "Min: 3.34e-10\n",
      "Max: 2.67e-01\n",
      "Mean: 6.67e-04\n",
      "Std: 4.27e-04\n",
      "\n",
      "Component 1:\n",
      "Min: -6.89e-04\n",
      "Max: 7.34e-03\n",
      "Mean: -1.48e-06\n",
      "Std: 8.80e-05\n",
      "\n",
      "Component 2:\n",
      "Min: 3.34e-10\n",
      "Max: 2.72e-01\n",
      "Mean: 6.93e-04\n",
      "Std: 4.32e-04\n",
      "\n",
      "Component 3:\n",
      "Min: -1.66e-02\n",
      "Max: 9.90e-04\n",
      "Mean: 8.52e-08\n",
      "Std: 9.10e-05\n",
      "\n",
      "Component 4:\n",
      "Min: -6.56e-03\n",
      "Max: 7.76e-04\n",
      "Mean: -1.75e-05\n",
      "Std: 9.18e-05\n",
      "\n",
      "Component 5:\n",
      "Min: 3.34e-10\n",
      "Max: 2.74e-01\n",
      "Mean: 6.75e-04\n",
      "Std: 4.36e-04\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Ground Truth Tensor Analysis:\")\n",
    "print(\"\\nComponent-wise statistics:\")\n",
    "for i in range(6):\n",
    "    comp = ground_truth_tensors[:, i]\n",
    "    print(f\"\\nComponent {i}:\")\n",
    "    print(f\"Min: {np.min(comp):.2e}\")\n",
    "    print(f\"Max: {np.max(comp):.2e}\")\n",
    "    print(f\"Mean: {np.mean(comp):.2e}\")\n",
    "    print(f\"Std: {np.std(comp):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
