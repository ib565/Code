{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = \"100206\"\n",
    "subject_path = f\"diffusion_data/{subject_id}/T1w/Diffusion\"\n",
    "dwi_img = nib.load(f'{subject_path}/data.nii.gz')\n",
    "mask_img = nib.load(f'{subject_path}/nodif_brain_mask.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for processing\n",
    "dwi_data = dwi_img.get_fdata()\n",
    "mask = mask_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWI data shape: (145, 174, 145, 288)\n",
      "Mask shape: (145, 174, 145)\n"
     ]
    }
   ],
   "source": [
    "print(f\"DWI data shape: {dwi_data.shape}\")  # Should be (X, Y, Z, num_volumes)\n",
    "print(f\"Mask shape: {mask.shape}\")          # Should be (X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading gradient information...\n",
      "Number of gradient directions: 288\n",
      "bvals shape: (288,)\n",
      "bvecs shape: (288, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load gradient information (bvals and bvecs)\n",
    "print(\"\\nLoading gradient information...\")\n",
    "bvals, bvecs = read_bvals_bvecs(f'{subject_path}/bvals', \n",
    "                               f'{subject_path}/bvecs')\n",
    "print(f\"Number of gradient directions: {len(bvals)}\")\n",
    "print(f\"bvals shape: {bvals.shape}\")     # Should match number of volumes\n",
    "print(f\"bvecs shape: {bvecs.shape}\")     # Should be (num_volumes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gradient table for DIPY\n",
    "gtab = gradient_table(bvals, bvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of B0 volumes: 18\n",
      "B0 data shape: (145, 174, 145, 18)\n",
      "Average B0 shape: (145, 174, 145)\n"
     ]
    }
   ],
   "source": [
    "# Identify and extract B0 (non-diffusion weighted) volumes\n",
    "b0_mask = gtab.b0s_mask\n",
    "b0_data = dwi_data[..., b0_mask]\n",
    "print(f\"\\nNumber of B0 volumes: {np.sum(b0_mask)}\")\n",
    "print(f\"B0 data shape: {b0_data.shape}\")\n",
    "# Average B0 volumes to get a single reference image\n",
    "b0_avg = np.mean(b0_data, axis=-1)\n",
    "print(f\"Average B0 shape: {b0_avg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of DWI volumes: 270\n",
      "DWI volumes shape: (145, 174, 145, 270)\n"
     ]
    }
   ],
   "source": [
    "# Extract and normalize diffusion weighted volumes\n",
    "dwi_mask = ~b0_mask  # Mask for diffusion weighted volumes\n",
    "dwi_vols = dwi_data[..., dwi_mask]\n",
    "print(f\"\\nNumber of DWI volumes: {np.sum(dwi_mask)}\")\n",
    "print(f\"DWI volumes shape: {dwi_vols.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized DWI shape: (145, 174, 145, 270)\n"
     ]
    }
   ],
   "source": [
    "# Normalize DWI volumes by B0 (avoid division by zero with small epsilon)\n",
    "dwi_norm = dwi_vols / (b0_avg[..., None] + 1e-6)\n",
    "print(f\"Normalized DWI shape: {dwi_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of valid voxels in mask: 936256\n"
     ]
    }
   ],
   "source": [
    "# Find valid voxels using the brain mask\n",
    "valid_idx = np.where(mask > 0)\n",
    "print(f\"\\nNumber of valid voxels in mask: {len(valid_idx[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random voxels for training\n",
    "n_samples = 50000  # Adjust this number based on your needs\n",
    "sample_idx = np.random.choice(len(valid_idx[0]), \n",
    "                            min(n_samples, len(valid_idx[0])), \n",
    "                            replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (signal intensities) from sampled voxels\n",
    "features = []\n",
    "for idx in sample_idx:\n",
    "    x, y, z = valid_idx[0][idx], valid_idx[1][idx], valid_idx[2][idx]\n",
    "    features.append(dwi_norm[x, y, z, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "gradient_directions = bvecs[dwi_mask]  # Only keep directions for DWI volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final data shapes:\n",
      "Features shape: (1000, 270)\n",
      "Gradient directions shape: (270, 3)\n",
      "\n",
      "Sanity checks:\n",
      "Max normalized value: 425303588.8671875\n",
      "Min normalized value: 0.0\n",
      "Gradient directions magnitude close to 1: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal data shapes:\")\n",
    "print(f\"Features shape: {features.shape}\")           # Should be (n_samples, n_directions)\n",
    "print(f\"Gradient directions shape: {gradient_directions.shape}\")  # Should be (n_directions, 3)\n",
    "\n",
    "# Basic sanity checks\n",
    "print(\"\\nSanity checks:\")\n",
    "print(f\"Max normalized value: {np.max(features)}\")\n",
    "print(f\"Min normalized value: {np.min(features)}\")\n",
    "print(f\"Gradient directions magnitude close to 1: {np.allclose(np.linalg.norm(gradient_directions, axis=1), 1, atol=1e-3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth data...\n",
      "Ground truth tensors shape: (936256, 6)\n",
      "Valid coordinates shape: (936256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading ground truth data...\")\n",
    "gt_data = np.load('ground_truth.npz')\n",
    "ground_truth_tensors = gt_data['tensors']\n",
    "valid_coordinates = gt_data['coordinates']\n",
    "print(f\"Ground truth tensors shape: {ground_truth_tensors.shape}\")\n",
    "print(f\"Valid coordinates shape: {valid_coordinates.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graphs for batch of 32 voxels using 41 directions each\n",
      "\n",
      "First sample edge check:\n",
      "Total edges created: 348\n",
      "Example edges and their angles:\n",
      "Edge 0: 0->3, Angle: 28.07 degrees\n",
      "Edge 1: 0->5, Angle: 12.20 degrees\n",
      "Edge 2: 0->6, Angle: 33.02 degrees\n",
      "Edge 3: 0->17, Angle: 23.30 degrees\n",
      "Edge 4: 0->19, Angle: 37.34 degrees\n",
      "\n",
      "Batch statistics:\n",
      "Number of samples in batch: 32\n",
      "Number of nodes per graph: 41\n",
      "Node feature dimensionality: 4\n",
      "Ground truth tensors: 32\n",
      "\n",
      "First sample in batch:\n",
      "Number of nodes: 41\n",
      "Number of edges: 348\n",
      "Ground truth tensor components: [ 4.26222250e-04 -2.06334230e-05  3.99538937e-04 -4.55641156e-06\n",
      " -5.93979584e-05  4.22905867e-04]\n"
     ]
    }
   ],
   "source": [
    "# Number of directions to use for sparse estimation\n",
    "n_directions = 41  # We can adjust this number\n",
    "\n",
    "def create_graph_data(features, gradient_directions, n_sparse_directions, batch_size = 32, threshold_angle = 45):\n",
    "    \"\"\"\n",
    "    Create graph data for a batch of samples, including ground tensor\n",
    "    \"\"\"\n",
    "    voxel_indices = np.random.choice(len(features), size = batch_size, replace = False)\n",
    "    print(f\"Creating graphs for batch of {batch_size} voxels using {n_sparse_directions} directions each\")\n",
    "    \n",
    "    batch_nodes = []\n",
    "    batch_edges = []\n",
    "    batch_tensors = []\n",
    "\n",
    "    for idx in voxel_indices:\n",
    "        # Randomly selected directions\n",
    "        selected_dir_idx = np.random.choice(len(gradient_directions), size=n_sparse_directions, replace=False)\n",
    "\n",
    "        directions = gradient_directions[selected_dir_idx]\n",
    "        signals = features[idx, selected_dir_idx]\n",
    "        \n",
    "        # Create node features: x, y, z, signal\n",
    "        nodes = np.column_stack([directions, signals])\n",
    "\n",
    "        # Create edges acc. to angle\n",
    "        directions_norm = directions / np.linalg.norm(directions, axis=1, keepdims=True)\n",
    "        cos_sim = np.dot(directions_norm, directions_norm.T)\n",
    "        angles = np.arccos(np.clip(cos_sim, -1.0, 1.0)) * 180/np.pi\n",
    "        \n",
    "        src, dst = np.where(angles < threshold_angle)\n",
    "        mask = src != dst\n",
    "        edges = np.column_stack([src[mask], dst[mask]])\n",
    "\n",
    "        batch_nodes.append(nodes)\n",
    "        batch_edges.append(edges)\n",
    "        batch_tensors.append(ground_truth_tensors[idx])\n",
    "\n",
    "        if idx == voxel_indices[0]:\n",
    "            print(\"\\nFirst sample edge check:\")\n",
    "            print(f\"Total edges created: {edges.shape[0]}\")\n",
    "            print(\"Example edges and their angles:\")\n",
    "            for i in range(min(5, len(edges))):\n",
    "                e1, e2 = edges[i]\n",
    "                print(f\"Edge {i}: {e1}->{e2}, Angle: {angles[e1,e2]:.2f} degrees\")\n",
    "        \n",
    "    return batch_nodes, batch_edges, batch_tensors\n",
    "\n",
    "nodes, edges, gt_tensors = create_graph_data(features, gradient_directions, n_directions)\n",
    "# Print info for verification\n",
    "print(\"\\nBatch statistics:\")\n",
    "print(f\"Number of samples in batch: {len(nodes)}\")\n",
    "print(f\"Number of nodes per graph: {nodes[0].shape[0]}\")\n",
    "print(f\"Node feature dimensionality: {nodes[0].shape[1]}\")\n",
    "print(f\"Ground truth tensors: {len(gt_tensors)}\")\n",
    "\n",
    "# Print example for first sample\n",
    "print(\"\\nFirst sample in batch:\")\n",
    "print(f\"Number of nodes: {nodes[0].shape[0]}\")\n",
    "print(f\"Number of edges: {edges[0].shape[0]}\")\n",
    "print(f\"Ground truth tensor components: {gt_tensors[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
